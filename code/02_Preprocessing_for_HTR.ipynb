{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f8d096-a20d-41b7-8b0b-506473cba516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df6d58c-eef3-49ae-acb6-a8f2740af974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_path=None, \n",
    "        output_path=None,\n",
    "        tvt=None,\n",
    "        crop=True,\n",
    "        rename=True,\n",
    "        create_lmdb=True,\n",
    "        get_gt=True\n",
    "    ):\n",
    "\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.tvt = tvt\n",
    "\n",
    "        self.create_dirs()\n",
    "        self.get_info()\n",
    "        \n",
    "        if crop:\n",
    "            self.crop()\n",
    "            \n",
    "        if rename:\n",
    "            self.rename()\n",
    "            \n",
    "        if create_lmdb:\n",
    "            self.create_lmdb()\n",
    "            \n",
    "        if get_gt:\n",
    "            self.get_gt()\n",
    "    \n",
    "    \n",
    "    def create_dirs(self):\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(f'{self.output_path}/{self.tvt}/lmdb')\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def get_info(self):\n",
    "\n",
    "        self.df_annotations = pd.read_csv(\n",
    "            f'{self.input_path}/{self.tvt}/_annotations.csv', \n",
    "            header=None, \n",
    "            names=['filename', 'x', 'y', 'x2', 'y2', 'label']\n",
    "            )\n",
    "\n",
    "        self.dimensions = []\n",
    "\n",
    "        for i in range(self.df_annotations.shape[0]):\n",
    "            self.dimensions.append(list(self.df_annotations.iloc[i, 1:5].values))\n",
    "\n",
    "        self.labels = list(self.df_annotations.label.values)\n",
    "\n",
    "        self.filenames = list(self.df_annotations.filename.values)\n",
    "\n",
    "        self.image_paths = [f'{self.input_path}/{self.tvt}/{filename}' for filename in self.filenames]\n",
    "\n",
    "        \n",
    "    def crop_image(self, image, box):\n",
    "    \n",
    "        self.image = image\n",
    "        self.box = box\n",
    "\n",
    "        self.x, self.y, self.x2, self.y2 = self.box\n",
    "        \n",
    "        self.cropped_image = self.image[self.y:self.y2, self.x:self.x2]\n",
    "        \n",
    "        return self.cropped_image\n",
    "\n",
    "    \n",
    "    def crop(self):\n",
    "\n",
    "        # Iterate over the predicted bounding boxes and crop the image\n",
    "        for image_path, dmns, label, filename in zip(\n",
    "            self.image_paths, \n",
    "            self.dimensions, \n",
    "            self.labels, \n",
    "            self.filenames):\n",
    "\n",
    "            self.cropped_image = self.crop_image(cv2.imread(image_path), dmns)\n",
    "\n",
    "            # Save the cropped image with the corresponding label as the filename\n",
    "            cv2.imwrite(f'{self.output_path}/{self.tvt}/{filename}_{dmns[-1]}_{label}.jpg', self.cropped_image)\n",
    "            \n",
    "            \n",
    "    def rename(self):\n",
    "\n",
    "        self.paths = os.listdir(f'{self.output_path}/{self.tvt}')\n",
    "\n",
    "        for i in range(len(self.paths)):\n",
    "\n",
    "            if self.paths[i].endswith('.jpg'):\n",
    "\n",
    "                self.new_filename = self.paths[i].split('_')[0] + \\\n",
    "                '_' + \\\n",
    "                self.paths[i].split('_')[-2] + \\\n",
    "                '_' + \\\n",
    "                '_'.join(self.paths[i].split('_')[-1].split(' '))\n",
    "\n",
    "                os.rename(\n",
    "                    os.path.join(f'{self.output_path}/{self.tvt}/', self.paths[i]), \n",
    "                    os.path.join(f'{self.output_path}/{self.tvt}/', self.new_filename)\n",
    "                )\n",
    "                \n",
    "                \n",
    "    def create_lmdb(self):\n",
    "\n",
    "        # Open the LMDB environment\n",
    "        self.env = lmdb.open(f'{self.output_path}/{self.tvt}/lmdb/', map_size=int(1e9))\n",
    "\n",
    "        # Start a write transaction\n",
    "        with self.env.begin(write=True) as self.txn:\n",
    "\n",
    "            for image_path in [\n",
    "                f'{self.output_path}/{self.tvt}/' + \\\n",
    "                x for x in os.listdir(f'{self.output_path}/{self.tvt}')\n",
    "            ]:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    # Read the image file\n",
    "                    image = cv2.imread(image_path)\n",
    "\n",
    "                    if image is None and image_path.endswith('.jpg'):\n",
    "                        raise ValueError(f\"Failed to read image: {image_path}\")\n",
    "\n",
    "                    # Convert the image to bytes\n",
    "                    image_bytes = cv2.imencode('.jpg', image)[1].tobytes()\n",
    "\n",
    "                    # Generate a unique key for the image\n",
    "                    key = image_path.encode('ascii')\n",
    "\n",
    "                    # Add the key-value pair to the LMDB database\n",
    "                    self.txn.put(key, image_bytes)\n",
    "\n",
    "                except Exception as e:\n",
    "                    \n",
    "                    if image_path.endswith('.jpg'):\n",
    "                    \n",
    "                        print(f\"Error processing image: {image_path}\")\n",
    "                        print(f\"Error message: {str(e)}\")\n",
    "\n",
    "        # Close the LMDB environment\n",
    "        self.env.close()\n",
    "        \n",
    "        \n",
    "    def get_gt(self):\n",
    "\n",
    "        self.path = f'{self.output_path}/{self.tvt}/'\n",
    "\n",
    "        self.info = {f'{self.path}{i}': ' '.join(i.split('_')[2:]).split('.')[0] for i in os.listdir(self.path)}\n",
    "\n",
    "        self.output_file = f'{self.output_path}/gt.txt'\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(self.output_file, 'a') as file:\n",
    "\n",
    "                for key, value in self.info.items():\n",
    "\n",
    "                    file.write(f'{key}\\t{value}\\n')\n",
    "\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            with open(self.output_file, 'w') as file:\n",
    "\n",
    "                for key, value in info.items():\n",
    "\n",
    "                    file.write(f'{key}\\t{value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99717717-828a-4868-9e5f-dd5bdf9ea91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lmdb(lmdb_path=None):\n",
    "\n",
    "    # Open the LMDB environment\n",
    "    env = lmdb.open(lmdb_path, readonly=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Start a read transaction\n",
    "    with env.begin() as txn:\n",
    "\n",
    "        # Open the LMDB database\n",
    "        db = txn.cursor()\n",
    "\n",
    "        # Iterate over the database and print the keys and values\n",
    "        for key, value in db:\n",
    "\n",
    "            try:\n",
    "\n",
    "                if count <= 1:\n",
    "\n",
    "                    key_str = key.decode('ascii')\n",
    "                    val_hex = value.hex()\n",
    "\n",
    "                    print(f'Key: {key_str}')\n",
    "                    print(f'Value: {val_hex[:100]}...')\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "\n",
    "                print('Error decoding key.')\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        print(count)\n",
    "\n",
    "    # Close the LMDB environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8735b2-d8b4-4b77-928e-8c4bac0f1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvt = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3157424-8010-443f-82a5-6ea0fb1de559",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../data/spelling_dictation/5_annotated_htr'\n",
    "output_path = '../data/spelling_dictation/6_cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544a0888-2608-47c5-bbba-f6c0a66e3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = False\n",
    "check_lmdb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf3748b-113e-47a0-956a-e3ff37d90741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: ../data/spelling_dictation/6_cropped/train/001_1245_worried.jpg\n",
      "Value: ffd8ffe000104a46494600010100000100010000ffdb00430002010101010102010101020202020204030202020205040403...\n",
      "Key: ../data/spelling_dictation/6_cropped/train/001_1400_haul.jpg\n",
      "Value: ffd8ffe000104a46494600010100000100010000ffdb00430002010101010102010101020202020204030202020205040403...\n",
      "1765\n",
      "\n",
      "Key: ../data/spelling_dictation/6_cropped/valid/003_1173_witnessed.jpg\n",
      "Value: ffd8ffe000104a46494600010100000100010000ffdb00430002010101010102010101020202020204030202020205040403...\n",
      "Key: ../data/spelling_dictation/6_cropped/valid/003_1497_pevemant.jpg\n",
      "Value: ffd8ffe000104a46494600010100000100010000ffdb00430002010101010102010101020202020204030202020205040403...\n",
      "503\n",
      "\n",
      "Key: ../data/spelling_dictation/6_cropped/test/008_1151_s_rface.jpg\n",
      "Value: ffd8ffe000104a46494600010100000100010000ffdb00430002010101010102010101020202020204030202020205040403...\n",
      "Key: ../data/spelling_dictation/6_cropped/test/008_1291_tare_ais.jpg\n",
      "Value: ffd8ffe000104a46494600010100000100010000ffdb00430002010101010102010101020202020204030202020205040403...\n",
      "258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tvt:\n",
    "    \n",
    "    if preprocess:\n",
    "        \n",
    "        Preprocess(\n",
    "            input_path=input_path,\n",
    "            output_path=output_path,\n",
    "            tvt=i,\n",
    "        )\n",
    "        \n",
    "    if check_lmdb:\n",
    "        \n",
    "        read_lmdb(lmdb_path=f'{output_path}/{i}/lmdb')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8a0ae-61c0-40ae-8011-cda8bba311a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
